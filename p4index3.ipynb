{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "p4index3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNvOLZeRqC+gYtyv+P/o8u6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jammy-bot/dsc-mod-4-project-v2-1-online-ds-ft-120919/blob/master/p4index3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2_ZF1iLQbck",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%html\n",
        "<marquee style='width: 50%; color: Green;'>It's a Kind of Magic!</marquee>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwcIf-GN6n_2",
        "colab_type": "text"
      },
      "source": [
        "Heading\n",
        "# Title\n",
        "## Subtitle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JilisXHhEINl",
        "colab_type": "text"
      },
      "source": [
        ">For this project, we will be working with the __Chest X-Ray Images (Pneumonia)__ dataset, from Kaggle [https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia](https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia). The objective of the project is to build a deep neural network model that can classify whether a given patient has pneumonia, given a chest x-ray image.\n",
        "\n",
        "        Acknowledgements\n",
        "        Data: https://data.mendeley.com/datasets/rscbjbr9sj/2\n",
        "\n",
        "        License: CC BY 4.0\n",
        "\n",
        "        Citation: http://www.cell.com/cell/fulltext/S0092-8674(18)30154-5\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLUdq737xC0D",
        "colab_type": "text"
      },
      "source": [
        "# Notebook Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HT2JZNzq05m",
        "colab_type": "text"
      },
      "source": [
        "Import required libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsR5BySzqxbV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, LabelBinarizer\n",
        "from keras.preprocessing.image import load_img, img_to_array, array_to_img\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras import layers, initializers, optimizers\n",
        "from keras.optimizers import SGD\n",
        "from keras.layers import Activation, Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from keras import backend as K\n",
        "\n",
        "import random\n",
        "from random import shuffle\n",
        "from tqdm import tqdm  \n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8C8JeeJF2XG9",
        "colab_type": "text"
      },
      "source": [
        "Verify the current working directory\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSQFyMq5gbkf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "!pwd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lpxo2aimu9rp",
        "colab_type": "text"
      },
      "source": [
        "Let's get a sense of our directory structure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiAEkdJ2xky7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# using Ipython magic to move to the `root` folder\n",
        "%cd /root/\n",
        "\n",
        "# viewing root subdirectories as a list\n",
        "# subdirs = [x[0] for x in os.walk('.')]\n",
        "# print(subdirs)\n",
        "\n",
        "# viewing root subdirectories in a vertical orientation\n",
        "#for x in os.walk('.'):\n",
        "#    print(x[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiV6iv_irHEF",
        "colab_type": "text"
      },
      "source": [
        "Create a `data` subdirectory and a hidden `.kaggle` directory for data download credentials."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yjj3k_JUgpoG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# creating a directory for sample data\n",
        "!mkdir ./data/\n",
        "\n",
        "# creating a directory for data download credentials\n",
        "!mkdir ~/.kaggle\n",
        "\n",
        "# list items in the `content` directory\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bXKxhbercAL",
        "colab_type": "text"
      },
      "source": [
        "Verify the hidden directory, and move into it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_gCncmCD0Rs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd ~/.kaggle/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0k8N0aT8wu5",
        "colab_type": "text"
      },
      "source": [
        "Upload your **Kaggle API key** to the current, hidden directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sEVzqkB84X8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEawisOYc1k4",
        "colab_type": "text"
      },
      "source": [
        "Verify the current directory and its contents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kq_s1jw0clsl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pwd\n",
        "%ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ierw1gkndG88",
        "colab_type": "text"
      },
      "source": [
        "__Update permissions__ on the json file to readable and writeabile by the owner and not readable, writeable, or executable by anyone else.\n",
        "\n",
        ">  Clear output from the `files.upload()` cell, above, before saving publicly the notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2ahIw3UO3vN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!chmod 600 ./kaggle.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SdPy_0AspVc",
        "colab_type": "text"
      },
      "source": [
        "Move to the `content/data` directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vqSTctOEULE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd ..\n",
        "%cd data/\n",
        "\n",
        "# viewing the current, `data` directory\n",
        "%ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FHSLYahzW8f",
        "colab_type": "text"
      },
      "source": [
        "# Obtain Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ju4KWlfGs3MC",
        "colab_type": "text"
      },
      "source": [
        ">The dataset is provided in \"3 folders (train, test, val) and contains subfolders for each image category (Pneumonia/Normal). There are 5,863 X-Ray images (JPEG) and 2 categories (Pneumonia/Normal).\"\n",
        "\n",
        "Download the dataset to the current `data`directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CQbFMu2oH-H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# downloading the dataset into `content/data'\n",
        "!kaggle datasets download -d paultimothymooney/chest-xray-pneumonia"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICYkwWKaqGy8",
        "colab_type": "text"
      },
      "source": [
        "Verify download."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgpJ0XRCqDeM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_C1aSMvsSfD",
        "colab_type": "text"
      },
      "source": [
        "Inflate the compressed files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmtrlYUauo-S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# extracting the zipped files and directories\n",
        "!unzip chest-xray-pneumonia.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "68hMN1j4quy5"
      },
      "source": [
        ">We now have a `chest_xray` folder with subdirectories for *training*, *validation*, and *test* data.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrfc3Ipavbjb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# verify directory contents\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdhW8SErCsAE",
        "colab_type": "text"
      },
      "source": [
        "\n",
        ">Data added to Google Colab projects get cleared at the close of each session, so we do not need to worry about removing the `chest-xray-pneumonia.zip` compressed file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "818W4zwu9HLo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# viewing 1 level deep\n",
        "print(os.listdir(\"/root/data/chest_xray/chest_xray\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9NXGK8R92z9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# viewing next level\n",
        "print(os.listdir(\"/root/data/chest_xray/chest_xray/val/\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLiITpCUy2VE",
        "colab_type": "text"
      },
      "source": [
        "# Scrub"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWyIDuAEvyDe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# instantiate variables for dataset directory paths\n",
        "train_dir = \"/root/data/chest_xray/chest_xray/train/\"\n",
        "train_normal = train_dir + 'NORMAL'\n",
        "train_pneumonia = train_dir + 'PNEUMONIA'\n",
        "\n",
        "val_dir = \"/root/data/chest_xray/chest_xray/val/\"\n",
        "val_normal = val_dir + 'NORMAL'\n",
        "val_pneumonia = val_dir + 'PNEUMONIA'\n",
        "\n",
        "test_dir = \"/root/data/chest_xray/chest_xray/test/\"\n",
        "test_normal = test_dir + 'NORMAL'\n",
        "test_pneumonia = test_dir + 'PNEUMONIA'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWFM0SYMwzbA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# instantiate variables for set lengths\n",
        "tn_normal_length = len([f for f in os.listdir(\n",
        "    train_normal) if f.endswith('.jpeg')])\n",
        "tn_pneumonial_length = len([f for f in os.listdir(\n",
        "    train_pneumonia) if f.endswith('.jpeg')])\n",
        "val_normal_length = len([f for f in os.listdir(\n",
        "    val_normal) if f.endswith('.jpeg')])\n",
        "val_pneumonia_length = len([f for f in os.listdir(\n",
        "    val_pneumonia) if f.endswith('.jpeg')])\n",
        "tt_normal_length = len([f for f in os.listdir(\n",
        "    test_normal) if f.endswith('.jpeg')])\n",
        "tt_pneumonia_length = len([f for f in os.listdir(\n",
        "    test_pneumonia) if f.endswith('.jpeg')])\n",
        "\n",
        "# printing how many images we have in each directory\n",
        "print('There are', tn_normal_length, \n",
        "'normal images in the training set\\n')\n",
        "\n",
        "print('There are', tn_pneumonial_length, \n",
        "'pneumonia images in the training set\\n')\n",
        "\n",
        "print('There are', val_normal_length, \n",
        "'normal images in the validation set\\n')\n",
        "\n",
        "print('There are', val_pneumonia_length, \n",
        "'pneumonia images in the validation set\\n')\n",
        "\n",
        "print('There are', tt_normal_length, \n",
        "'normal images in the test set\\n')\n",
        "\n",
        "print('There are', tt_pneumonia_length, \n",
        "'pneumonia images in the test set\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuYdCzqBehXZ",
        "colab_type": "text"
      },
      "source": [
        ">* Pneumonia images surpass the number of normal images nearly 3-fold, in the training data set.\n",
        "* Altogether, there are more than 5000 training images, 16 validation images, and 624 test images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxiXA00vvCP2",
        "colab_type": "text"
      },
      "source": [
        "# Explore and Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4yoxeYd4C-n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# importing necessary libraries.\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from scipy import ndimage\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "\n",
        "np.random.seed(123)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4VuQ5y7TKUa",
        "colab_type": "text"
      },
      "source": [
        "__Data Counts and Shapes__\n",
        "\n",
        "__Standardize__ image data by dividing each matrix by 255 and resizing images to 150 x 150."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-QlZvfzNX9K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# instantiating rescaling generators tor train and val data\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dGrNhBTEKbf",
        "colab_type": "text"
      },
      "source": [
        "__Reshape__ image data and labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-G-b0EfFOsdd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# generating training and validation data\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        # This is the target directory\n",
        "        train_dir,\n",
        "        # All images will be resized to 150x150\n",
        "        target_size=(150, 150),\n",
        "        batch_size=20,\n",
        "        # Since we use binary_crossentropy loss, we need binary labels\n",
        "        class_mode='binary')\n",
        "validation_generator = test_datagen.flow_from_directory(val_dir,\n",
        "                                                        target_size=(150, 150),\n",
        "                                                        batch_size=20,\n",
        "                                                        class_mode='binary')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PLnMTmL-cBC",
        "colab_type": "text"
      },
      "source": [
        "__Visualize Training Data Counts__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cj4RYrp6MKY6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bar_plot(dir):\n",
        "    x_dir = os.listdir(dir+'/NORMAL')\n",
        "    y_dir = os.listdir(dir+'/PNEUMONIA')\n",
        "    x=len(x_dir)\n",
        "    y=len(y_dir)\n",
        "    if dir != test_dir: \n",
        "        # account for non - image files in `train` and `val` directories\n",
        "        x = x - 1\n",
        "        y = y - 1\n",
        "    print(f'{dir} Images:\\n')\n",
        "    print('NORMAL:', x)\n",
        "    print('PNEUMONIA:', y)\n",
        "    print('Total images:', x + y)\n",
        "    print('-'*50)\n",
        "    # instantiate directory name for plot title\n",
        "    subdir_title = str.split(dir, '/')[-2] + ' directory'\n",
        "    category = ['NORMAL', 'PNEUMONIA']\n",
        "    count = [x, y]\n",
        "    plot = plt.bar(category,count) \n",
        "    plot[0].set_color('orange')\n",
        "    plt.title(\n",
        "        f\"Number of values per category in {subdir_title.title()}\\n\")\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aigbdWXp-d4W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plotting training data counts\n",
        "bar_plot(train_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PDX6SaYVcg8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plotting validation data counts\n",
        "bar_plot(val_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQgzQYWIVmoC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plotting test data counts\n",
        "bar_plot(test_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kpok2WL_AlTz",
        "colab_type": "text"
      },
      "source": [
        "### Preview Images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCFs0rvNCydg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# instantiate variables for a training image from each class\n",
        "normal_example = os.listdir(f'{train_dir}/NORMAL')[33]\n",
        "pneumonia_example = os.listdir(f'{train_dir}/PNEUMONIA')[33]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NL2jNj8aHMr5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot a normal class training image\n",
        "normal_img = plt.imread(f'{train_dir}/NORMAL/{normal_example}')\n",
        "plt.title(\"normal\")\n",
        "plt.imshow(normal_img)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14nKRI0XAicz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot a pneumonia class training image\n",
        "pneumonia_img = plt.imread(f'{train_dir}/PNEUMONIA/{pneumonia_example}')\n",
        "plt.imshow(pneumonia_img)\n",
        "plt.title('pneumonia')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPWhdC6rNYFh",
        "colab_type": "text"
      },
      "source": [
        "## Design Model 1\n",
        ">Using Keras:\n",
        "* Alternate convolutional and pooling layers\n",
        "* Include later layers with a larger number of parameters in order to detect more abstract patterns\n",
        "* Add final dense layers to add a classifier to the convolutional base\n",
        "* Compile this model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ErDDAA7UQj8u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# importing necessary libraries\n",
        "from keras import layers\n",
        "from keras import models\n",
        "from keras import optimizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSQkIMwwQtLV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# building the CNN model\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
        "                        input_shape=(150, 150, 3)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(512, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEnlIrFuRT6_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# compiling the model\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=optimizers.RMSprop(lr=1e-4),\n",
        "              metrics=['acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXZ39itcXh_L",
        "colab_type": "text"
      },
      "source": [
        ">Import keras modules: \n",
        "* to stop training when a monitored quantity has stopped improving\n",
        "* to save the model after every epoch\n",
        "* to reduce learning rate when a metric has stopped improving\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHNTiySLVa9Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# importing keras modules\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import ReduceLROnPlateau"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlZuQYEacNED",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# creating a directory for saving models\n",
        "!mkdir ./models/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbxJvCjgYCZU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# creating callback checkpoints\n",
        "f_path = './models/'\n",
        "my_callbacks = [\n",
        "    EarlyStopping(patience=4, verbose=1),\n",
        "    ReduceLROnPlateau(factor=0.1, patience=3, min_lr=0.00001, verbose=1),\n",
        "    ModelCheckpoint(filepath = f_path + 'p3_model.h5', \n",
        "    verbose=1, save_best_only=True, save_weights_only=False) \n",
        "    ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LZi8FlkR0yU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# training and evaluating the model\n",
        "# using the callback in the `model.fit`\n",
        "history = model.fit_generator(train_generator, \n",
        "                              steps_per_epoch=100, \n",
        "                              epochs=30, \n",
        "                              validation_data=validation_generator, \n",
        "                              validation_steps=50, \n",
        "                              callbacks = my_callbacks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhKNQ3EAmPk8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmByc2lhc8U7",
        "colab_type": "text"
      },
      "source": [
        "__Visualize Training Results__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeMmPbp0SyPy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline \n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(len(acc))\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy along epochs')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss along epochs')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RU1pfs93ehJX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# download the model\n",
        "files.download('./models/p3_model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mBERJgt0xqSw",
        "colab": {}
      },
      "source": [
        "val_loss, val_acc = model.evaluate_generator(validation_generator, steps=50)\n",
        "\n",
        "print('val loss:', val_loss)\n",
        "print('val acc:', val_acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyYtK3LYaYnj",
        "colab_type": "text"
      },
      "source": [
        "### Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5tL4QSMm1vy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# generating transformed data\n",
        "train_datagen = ImageDataGenerator(rotation_range=40, \n",
        "                                   width_shift_range=0.2, \n",
        "                                   height_shift_range=0.2, \n",
        "                                   shear_range=0.2, \n",
        "                                   zoom_range=0.2, \n",
        "                                   horizontal_flip=True, \n",
        "                                   fill_mode='nearest')\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        # This is the target directory\n",
        "        train_dir,\n",
        "        # All images will be resized to 150x150\n",
        "        target_size=(150, 150),\n",
        "        batch_size=32,\n",
        "        # Since we use binary_crossentropy loss, we need binary labels\n",
        "        class_mode='binary')\n",
        "\n",
        "# transforming only image size, for validation data\n",
        "validation_generator = test_datagen.flow_from_directory(val_dir, \n",
        "                                                        target_size=(150, 150), \n",
        "                                                        batch_size=32, \n",
        "                                                        class_mode='binary')\n",
        "\n",
        "# fitting a model to training set including transformations\n",
        "history = model.fit_generator(train_generator, \n",
        "                              steps_per_epoch=100, \n",
        "                              epochs=100, \n",
        "                              validation_data=validation_generator, \n",
        "                              validation_steps=50, \n",
        "                              callbacks = my_callbacks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPIqco9yq_Dc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_loss, val_acc = model.evaluate_generator(validation_generator, steps=50)\n",
        "\n",
        "print('val loss:', val_loss)\n",
        "print('val acc:', val_acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6OK5r8PK62v",
        "colab_type": "text"
      },
      "source": [
        "> No significant change"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFTpk3hzooq9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(len(acc))\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy along epochs')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss along epochs')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0r6nhlrppA6O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# download the model\n",
        "files.download('./models/p3_model.h5')\n",
        "\n",
        "# summarize the model\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WG_hrT5pYsjl",
        "colab_type": "text"
      },
      "source": [
        "__Plot Confusion Matrix__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Nz_eH6wYq1r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from sklearn.metrics import confusion_matrix\n",
        "y_true=validation_generator.classes\n",
        "print(y_true)\n",
        "\n",
        "y_pred = model.predict_generator(validation_generator)\n",
        "y_pred = np.argmax(y_pred,axis = 1)\n",
        "print(y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pjLi_vA6Ktx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from mlxtend.plotting import plot_confusion_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jrxj6ugciKw2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CM = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "fig, ax = plot_confusion_matrix(conf_mat=CM , \n",
        "                                figsize=(7, 7), \n",
        "                                hide_ticks=True, \n",
        "                                cmap=plt.cm.Blues)\n",
        "# plt.xticks(range(len(classes)), classes, fontsize=12)\n",
        "# plt.yticks(range(len(classes)), classes, fontsize=12)\n",
        "plt.title(\"Confusion Matrix for ...: \\n\") #+model_title, fontsize=11\n",
        "# fig.savefig(image_file_name_CM, dpi=100)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLeTqNEmusoi",
        "colab_type": "text"
      },
      "source": [
        "### Evaluate Model 1 on Test Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1dtUOwrr7Bv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_generator = test_datagen.flow_from_directory(test_dir, \n",
        "                                                  target_size=(150, 150), \n",
        "                                                  batch_size=20, \n",
        "                                                  class_mode='binary')\n",
        "\n",
        "test_loss, test_acc = model.evaluate_generator(test_generator, steps=50)\n",
        "\n",
        "print('test loss:', test_loss)\n",
        "print('test acc:', test_acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEJuRK7kb3Ra",
        "colab_type": "text"
      },
      "source": [
        "> So the first model, with augmentation, has achieved __78.56\\%__ accuracy predicting the class ('normal' or 'pneumonia' of our test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydQNf-UF4RxE",
        "colab_type": "text"
      },
      "source": [
        "## Model 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nAIoozX5lGW",
        "colab_type": "text"
      },
      "source": [
        "* add padding to input layer\n",
        "* add dropout layers after each pooling layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RtjG0-E34S4W",
        "colab": {}
      },
      "source": [
        "# building the a 2nd CNN model\n",
        "model_2 = models.Sequential()\n",
        "model_2.add(layers.Conv2D(32, (3, 3), activation='relu', padding=\"same\",\n",
        "                        input_shape=(150, 150, 3)))\n",
        "model_2.add(layers.MaxPooling2D((2, 2)))\n",
        "model_2.add(Dropout(0.25))\n",
        "model_2.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model_2.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model_2.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model_2.add(layers.MaxPooling2D((2, 2)))\n",
        "model_2.add(Dropout(0.25))\n",
        "model_2.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model_2.add(layers.MaxPooling2D((2, 2)))\n",
        "model_2.add(Dropout(0.25))\n",
        "model_2.add(layers.Flatten() )# converts 3D feature maps to 1D vectors\n",
        "model_2.add(Dense(64))\n",
        "model_2.add(Activation('relu'))\n",
        "model_2.add(layers.Dense(512, activation='relu'))\n",
        "model_2.add(layers.Dense(1, activation='sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBhud55m7u4F",
        "colab_type": "text"
      },
      "source": [
        "* Change the optimizer to `Adam`\n",
        "* use 'accuracy' metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WQb6HiK94S4d",
        "colab": {}
      },
      "source": [
        "# compiling the model\n",
        "model_2.compile(loss='binary_crossentropy',\n",
        "              optimizer=optimizers.Adam(lr=1e-4),\n",
        "              metrics=['acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ovvWRRAm4S4g"
      },
      "source": [
        ">Import keras modules: \n",
        "* to stop training when a monitored quantity has stopped improving\n",
        "* to save the model after every epoch\n",
        "* to reduce learning rate when a metric has stopped improving\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jOCQ7z2p4S4h",
        "colab": {}
      },
      "source": [
        "# importing keras modules\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import ReduceLROnPlateau"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fyLTugP64S4l",
        "colab": {}
      },
      "source": [
        "# creating callback checkpoints\n",
        "f_path = './models/'\n",
        "my_callbacks = [\n",
        "    EarlyStopping(patience=4, verbose=1),\n",
        "    ReduceLROnPlateau(factor=0.1, patience=3, min_lr=0.00001, verbose=1),\n",
        "    ModelCheckpoint(filepath = f_path + 'p3_model_2.h5', \n",
        "    verbose=1, save_best_only=True, save_weights_only=False) \n",
        "    ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "F7vQOzi04S4n",
        "colab": {}
      },
      "source": [
        "# training and evaluating the model\n",
        "# using the callback in the `model.fit`\n",
        "history_2 = model_2.fit_generator(train_generator, \n",
        "                              steps_per_epoch=100, \n",
        "                              epochs=30, \n",
        "                              validation_data=validation_generator, \n",
        "                              validation_steps=50, \n",
        "                              callbacks = my_callbacks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7vrWLCeJ4S4s",
        "colab": {}
      },
      "source": [
        "model_2.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "opO480q14S4t"
      },
      "source": [
        "__Visualize Training Results__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Jpj-X33v4S4u",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline \n",
        "acc = history_2.history['acc']\n",
        "val_acc = history_2.history['val_acc']\n",
        "loss = history_2.history['loss']\n",
        "val_loss = history_2.history['val_loss']\n",
        "epochs = range(len(acc))\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy along epochs')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss along epochs')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eOqUJwtm4S4p",
        "colab": {}
      },
      "source": [
        "# download the model\n",
        "files.download('./models/p3_model_2.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YVlHANoP4S4w",
        "colab": {}
      },
      "source": [
        "val_loss, val_acc = model_2.evaluate_generator(validation_generator, \n",
        "                                               steps=50)\n",
        "\n",
        "print('val loss:', val_loss)\n",
        "print('val acc:', val_acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzujLzTqcsX6",
        "colab_type": "text"
      },
      "source": [
        "> Early stopping may have been a bit premature, for this model, as measures do not appear to have settled into a definitive trend."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oo-CiZ6tdUDy"
      },
      "source": [
        "__Plot Confusion Matrix__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nq5wDuVXdUD0",
        "colab": {}
      },
      "source": [
        "# from sklearn.metrics import confusion_matrix\n",
        "y_true=validation_generator.classes\n",
        "print(y_true)\n",
        "\n",
        "y_pred = model_2.predict_generator(validation_generator)\n",
        "y_pred = np.argmax(y_pred,axis = 1)\n",
        "print(y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPOu2SFpLR6P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CM = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "fig, ax = plot_confusion_matrix(conf_mat=CM , \n",
        "                                figsize=(7, 7), \n",
        "                                hide_ticks=True, \n",
        "                                cmap=plt.cm.Blues)\n",
        "# plt.xticks(range(len(classes)), classes, fontsize=12)\n",
        "# plt.yticks(range(len(classes)), classes, fontsize=12)\n",
        "plt.title(\"Confusion Matrix for ...: \\n\") #+model_title, fontsize=11\n",
        "# fig.savefig(image_file_name_CM, dpi=100)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dT1ddJv4Lfjo",
        "colab_type": "text"
      },
      "source": [
        "### Evaluate Model 2 on Test Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yz370mOJPa3M",
        "colab": {}
      },
      "source": [
        "test_generator = test_datagen.flow_from_directory(test_dir, \n",
        "                                                  target_size=(150, 150), \n",
        "                                                  batch_size=20, \n",
        "                                                  class_mode='binary')\n",
        "\n",
        "test_loss, test_acc = model_2.evaluate_generator(test_generator, steps=50)\n",
        "\n",
        "print('test loss:', test_loss)\n",
        "print('test acc:', test_acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4m1u_4-adGjj"
      },
      "source": [
        ">The second model performed poorly, with a test set prediction accuracy score ~__16\\%__ lower than the first model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EX50HQY_AiIo",
        "colab_type": "text"
      },
      "source": [
        "## Model 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BWK1ggNRAk9z"
      },
      "source": [
        "* trying fewer layers and fewer dropouts\n",
        "* add dropout layers after each pooling layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S1WErkU6Ak9z",
        "colab": {}
      },
      "source": [
        "# building the a 2nd CNN model\n",
        "model_3 = models.Sequential()\n",
        "model_3.add(layers.Conv2D(32, (3, 3), activation='relu', padding=\"same\",\n",
        "                        input_shape=(150, 150, 3)))\n",
        "model_3.add(layers.MaxPooling2D((2, 2)))\n",
        "model_3.add(Dropout(0.25))\n",
        "model_3.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model_3.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model_3.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model_3.add(layers.MaxPooling2D((2, 2)))\n",
        "model_3.add(layers.Flatten() )# this converts 3D feature maps to 1D vectors\n",
        "model_3.add(layers.Dense(512, activation='relu'))\n",
        "model_3.add(layers.Dense(1, activation='sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "y5szmwHbAk93"
      },
      "source": [
        "* Change the optimizer to `Adam`\n",
        "* use 'accuracy' metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rkoUIE4yAk94",
        "colab": {}
      },
      "source": [
        "# compiling the model\n",
        "model_3.compile(loss='binary_crossentropy',\n",
        "              optimizer=optimizers.Adam(lr=1e-4),\n",
        "              metrics=['acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "s6Nn2UcOAk96"
      },
      "source": [
        ">Import keras modules: \n",
        "* to stop training when a monitored quantity has stopped improving\n",
        "* to save the model after every epoch\n",
        "* to reduce learning rate when a metric has stopped improving\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7lDxO7jZAk97",
        "colab": {}
      },
      "source": [
        "# importing keras modules\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import ReduceLROnPlateau"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fC6OfhHBtBms",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# increase early stopping patience"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VtF6Sfk6Ak9-",
        "colab": {}
      },
      "source": [
        "# updating callback checkpoints\n",
        "f_path = './models/'\n",
        "my_callbacks = [\n",
        "    EarlyStopping(patience=7, verbose=1),\n",
        "    ReduceLROnPlateau(factor=0.1, patience=3, min_lr=0.00001, verbose=1),\n",
        "    ModelCheckpoint(filepath = f_path + 'p3_model_3.h5', \n",
        "    verbose=1, save_best_only=True, save_weights_only=False) \n",
        "    ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Uqs-AYq1Ak-B",
        "colab": {}
      },
      "source": [
        "# training and evaluating the model\n",
        "# using the callback in the `model.fit`\n",
        "history_3 = model_3.fit_generator(train_generator, \n",
        "                              steps_per_epoch=100, \n",
        "                              epochs=30, \n",
        "                              validation_data=validation_generator, \n",
        "                              validation_steps=50, \n",
        "                              callbacks = my_callbacks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ql8mOO-CAk-G",
        "colab": {}
      },
      "source": [
        "model_3.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_FbNna26Ak-I"
      },
      "source": [
        "__Visualize Training Results__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "virbd0RwAk-J",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline \n",
        "acc = history_3.history['acc']\n",
        "val_acc = history_3.history['val_acc']\n",
        "loss = history_3.history['loss']\n",
        "val_loss = history_3.history['val_loss']\n",
        "epochs = range(len(acc))\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy along epochs')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss along epochs')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CC4UrjX3Ak-M",
        "colab": {}
      },
      "source": [
        "val_loss, val_acc = model_3.evaluate_generator(validation_generator, \n",
        "                                               steps=50)\n",
        "\n",
        "print('val loss:', val_loss)\n",
        "print('val acc:', val_acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMnH1ARzeprv",
        "colab_type": "text"
      },
      "source": [
        "> Acurracy droped from the previous model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sP23jqYydvsa"
      },
      "source": [
        "__Plot Confusion Matrix__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UsVNyo0Cdvsb",
        "colab": {}
      },
      "source": [
        "# from sklearn.metrics import confusion_matrix\n",
        "y_true=validation_generator.classes\n",
        "print(y_true)\n",
        "\n",
        "y_pred = model_3.predict_generator(validation_generator)\n",
        "y_pred = np.argmax(y_pred,axis = 1)\n",
        "print(y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PH_2PV2__lPl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CM = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "fig, ax = plot_confusion_matrix(conf_mat=CM , \n",
        "                                figsize=(7, 7), \n",
        "                                hide_ticks=True, \n",
        "                                cmap=plt.cm.Blues)\n",
        "# plt.xticks(range(len(classes)), classes, fontsize=12)\n",
        "# plt.yticks(range(len(classes)), classes, fontsize=12)\n",
        "plt.title(\"Confusion Matrix for ...: \\n\") #+model_title, fontsize=11\n",
        "# fig.savefig(image_file_name_CM, dpi=100)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "L5xxfgOGAk-E",
        "colab": {}
      },
      "source": [
        "# download the model\n",
        "files.download('./models/p3_model_3.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mfI6pTbAFqJS"
      },
      "source": [
        "### Evaluate Model 3 on Test Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "U4WXr3ZEFqJT",
        "colab": {}
      },
      "source": [
        "test_generator = test_datagen.flow_from_directory(test_dir, \n",
        "                                                  target_size=(150, 150), \n",
        "                                                  batch_size=20, \n",
        "                                                  class_mode='binary')\n",
        "\n",
        "test_loss, test_acc = model_3.evaluate_generator(test_generator, steps=50)\n",
        "\n",
        "print('test loss:', test_loss)\n",
        "print('test acc:', test_acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LG3hOcjfIWk",
        "colab_type": "text"
      },
      "source": [
        ">The accuracy score for test data is even (slightly) lower than that of the second model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKZCFJLMKLFZ",
        "colab_type": "text"
      },
      "source": [
        "# Select a Final Model\n",
        "\n",
        "> It turns out that our best performing model was our first augmented model, with RMSprop optimization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9zylGvJKOjF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# evaluating the best model on test data\n",
        "# setting steps to 39 and batch_size to 16 (for 624 images)\n",
        "test_generator = test_datagen.flow_from_directory(test_dir, \n",
        "                                                  target_size=(150, 150), \n",
        "                                                  batch_size=16, \n",
        "                                                  class_mode='binary')\n",
        "\n",
        "test_loss, test_acc = model.evaluate_generator(test_generator, steps=39)\n",
        "\n",
        "print('test loss:', test_loss)\n",
        "print('test acc:', test_acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoetEZGxiFY0",
        "colab_type": "text"
      },
      "source": [
        "* We can use this model as a basis for further tuning, or distribute the model as - is.\n",
        "* A potential approach would be to perform a gridsearch on the optimizer, to determine best parameters.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upELSyddR9xy",
        "colab_type": "text"
      },
      "source": [
        "## Distribution\n",
        "\n",
        "* Since we previously saved our `my_model.h5` model, we can distribute it and use it on another system or for another project, as follows.\n",
        "* The string summary can be saved and shared, as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGW-Es_ql00T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loading the saved model\n",
        "from keras.models import load_model\n",
        "\n",
        "filepath = '/root/data/models/p3_model.h5' # current path to saved file\n",
        "\n",
        "dist_model = load_model(filepath, \n",
        "         custom_objects={'loss':'binary_crossentropy'})\n",
        "\n",
        "# printing string summary\n",
        "dist_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HI6XLzkH1nxf",
        "colab_type": "text"
      },
      "source": [
        "The model's architecture may be shared as an image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vBq2_l0z7z0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils import plot_model\n",
        "\n",
        "# plot model architecture, only\n",
        "plot_model(dist_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGOXMuZNg7kF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# evaluating the distributed model on test data\n",
        "# setting steps to 39 and batch_size to 16 (for 624 images)\n",
        "test_generator = test_datagen.flow_from_directory(test_dir, \n",
        "                                                  target_size=(150, 150), \n",
        "                                                  batch_size=16, \n",
        "                                                  class_mode='binary')\n",
        "\n",
        "test_loss, test_acc = dist_model.evaluate_generator(test_generator, \n",
        "                                                    steps=39)\n",
        "\n",
        "print('Restored model, loss:', test_loss)\n",
        "print('Restored model, acc:', test_acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPeg6cWCsXB4",
        "colab_type": "text"
      },
      "source": [
        "> Somehow, the restored model reports an even better performance than we realized when it was first run (> __86.06\\%__ vs. __83.97\\%__). \n",
        "* One apparent difference is that we used 39 steps to evaluate the reloaded model, where we originally had used 50 steps.\n",
        "* I also included a 'ModelCheckpoint' callback that saved the model only after epochs where loss improved.\n",
        "* Also worth of exploration is the effect of the lack of dropout layers in testing. \n",
        ">\n",
        ">For now, let's re - check the distributed model against the test data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctHBewU6KgXN",
        "colab_type": "text"
      },
      "source": [
        "# Summary"
      ]
    }
  ]
}